{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2d7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by: Danish Zaheer\n",
    "#Date: 17-10-2022\n",
    "\n",
    "#importing all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d6f0c",
   "metadata": {},
   "source": [
    "### **Problem 2.1:**\n",
    "\n",
    "Use Ridge regression to solve the regression problem in Example 2.1 as well as the classification problem in Example 2.2, also implement both closed-form and iterative approachs, compare the results of Ridge regression with those of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fd115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "#importing data\n",
    "raw_data = pd.read_csv('boston.csv', header=None)\n",
    "#reshaping raw data\n",
    "data_rows = np.reshape(raw_data.to_numpy(), (506,14))\n",
    "#separating features\n",
    "data = data_rows[:,:13]\n",
    "#separting output\n",
    "target = data_rows[:,13]\n",
    "# normalize input features to zero-mean and unit-variance\n",
    "data = (data-np.mean(data, axis=0))/np.std(data, axis=0) \n",
    "print(data.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534f4f5",
   "metadata": {},
   "source": [
    "\n",
    "**Implementing closed-form**\n",
    "Ridge regression = (XTX+λI)^−1(XTy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecad6b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error for the closed-form Ridge regression solution: 21.89781\n"
     ]
    }
   ],
   "source": [
    "# add a constant column of '1' to accomodate the bias \n",
    "data_wb = np.hstack((data, np.ones((data.shape[0], 1), dtype=data.dtype)))\n",
    "I = np.eye(data_wb.shape[1])\n",
    "I[0][0] = 0 \n",
    "alpha = 1\n",
    "#closed-form solution\n",
    "w = np.linalg.inv(data_wb.T @ data_wb + alpha*I) @ data_wb.T @ target\n",
    "# calculate the mean square error in the training set\n",
    "predict = data_wb @ w \n",
    "error = np.sum((predict - target)*(predict - target))/data.shape[0]\n",
    "e1 = error #for further comparison\n",
    "print(f'Mean square error for the closed-form Ridge regression solution: {error:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187d8cb",
   "metadata": {},
   "source": [
    "**Implementing Iterative Ridge regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab1183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression(alternative function for testing purpose)\n",
    "class RidgeRegression():\n",
    "    def __init__(self, learning_rate, iterations, L2_penality):  \n",
    "        self.learning_rate = learning_rate        \n",
    "        self.iterations = iterations        \n",
    "        self.L2_penality = L2_penality\n",
    "    # Function for model training            \n",
    "    def fit(self, X, Y ):\n",
    "        # no_of_training_examples, no_of_features        \n",
    "        self.m, self.n = X.shape\n",
    "        # weight initialization        \n",
    "        self.W = np.zeros( self.n)\n",
    "        self.b = 0        \n",
    "        self.X = X        \n",
    "        self.Y = Y\n",
    "        # gradient descent learning\n",
    "        for i in range(self.iterations):            \n",
    "            self.update_weights()            \n",
    "        return self\n",
    "    # Helper function to update weights in gradient descent\n",
    "    def update_weights(self):           \n",
    "        Y_pred = self.predict(self.X)\n",
    "        # calculate gradients\n",
    "        dW = (-(2*(self.X.T).dot(self.Y - Y_pred)) + (2*self.L2_penality*self.W))/self.m     \n",
    "        db = -2*np.sum(self.Y - Y_pred)/self.m \n",
    "        # update weights    \n",
    "        self.W = self.W - self.learning_rate * dW    \n",
    "        self.b = self.b - self.learning_rate * db        \n",
    "        return self\n",
    "    # Hypothetical function  h( x ) \n",
    "    def predict(self, X) :    \n",
    "        return X.dot( self.W ) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3a91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve ridge regression using gradient descent \n",
    "class Optimizer():\n",
    "    def __init__(self, lr, alpha, annealing_rate, batch_size, max_epochs):\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.annealing_rate = annealing_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "# X[N,d]: input features; y[N]: output targets; op: hyper-parameters for optimzer \n",
    "def ridge_regression_gd(X, y, op):\n",
    "    n = X.shape[0]   # number of samples\n",
    "    w = np.zeros(X.shape[1])  # initialization\n",
    "    lr = op.lr\n",
    "    errors = np.zeros(op.max_epochs)\n",
    "    \n",
    "    for epoch in range(op.max_epochs):\n",
    "        indices = np.random.permutation(n)  #randomly shuffle data indices\n",
    "        for batch_start in range(0, n, op.batch_size):\n",
    "            X_batch = X[indices[batch_start:batch_start + op.batch_size]]\n",
    "            y_batch = y[indices[batch_start:batch_start + op.batch_size]]\n",
    "            \n",
    "            # vectorization to compute gradients for a whole mini-batch (see the above formula)\n",
    "            w_grad = (-2/y_batch.size)*X_batch.T.dot((y_batch - X_batch@w)) + op.alpha*w \n",
    "            \n",
    "            w -= lr * w_grad / X_batch.shape[0]\n",
    "        diff = X @ w - y  # prediction difference\n",
    "        errors[epoch] = np.sum(diff*diff)/n\n",
    "        lr *= op.annealing_rate\n",
    "        #print(f'epoch={epoch}: the mean square error is {errors[epoch]}')\n",
    "        \n",
    "    return w, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40680d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error Iterative Ridge regression: 22.10873\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "op = Optimizer(lr=0.1, alpha=0.001, annealing_rate=0.99, batch_size=30 , max_epochs=500)\n",
    "w, errors = ridge_regression_gd(data_wb, target, op)\n",
    "#Training the model\n",
    "predict = data_wb @ w\n",
    "#predicting MSE\n",
    "print(\"Mean squared error Iterative Ridge regression: %.5f\" % mean_squared_error(target, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14451247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error Iterative Ridge regression: 21.96028\n"
     ]
    }
   ],
   "source": [
    "#For testing purpose please ignore\n",
    "\n",
    "# hyperparameters\n",
    "#op = RidgeRegression(iterations = 1000, learning_rate = 0.01, L2_penality = 1)\n",
    "#Training the model\n",
    "#op.fit( data_wb, target)\n",
    "#predict = op.predict(data_wb)\n",
    "#predicting MSE\n",
    "#print(\"Mean squared error Iterative Ridge regression: %.5f\" % mean_squared_error(target, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5385b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for linear regression: 21.89483\n",
      "Mean squared error for Ridge regression: 21.89586\n"
     ]
    }
   ],
   "source": [
    "# Comparison with Linear regression\n",
    "\n",
    "# Create linear regression object\n",
    "l_regr = linear_model.LinearRegression()\n",
    "# Train the model using the training set\n",
    "l_regr.fit(data_wb, target)\n",
    "# Make predictions using the same training set\n",
    "predict = l_regr.predict(data_wb)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error for linear regression: %.5f\" % mean_squared_error(target, predict))\n",
    "\n",
    "# Using Ridge regression Scikit learn\n",
    "l_Ridge = linear_model.Ridge(alpha = 1)\n",
    "# Train the model using the training set\n",
    "l_Ridge.fit(data_wb, target)\n",
    "# Make predictions using the same training set\n",
    "predict = l_Ridge.predict(data_wb)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error for Ridge regression: %.5f\" % mean_squared_error(target, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de04b6d",
   "metadata": {},
   "source": [
    "**Solving Example 2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dcce2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from mnist import MNIST\n",
    "\n",
    "mnist_loader = MNIST('MNIST')\n",
    "train_data, train_label = mnist_loader.load_training()\n",
    "test_data, test_label = mnist_loader.load_testing()\n",
    "train_data = np.array(train_data, dtype='float')/255 # norm to [0,1]\n",
    "train_label = np.array(train_label, dtype='short')\n",
    "test_data = np.array(test_data, dtype='float')/255 # norm to [0,1]\n",
    "test_label = np.array(test_label, dtype='short')\n",
    "#add small random noise to avoid matrix singularity\n",
    "train_data += np.random.normal(0,0.0001,train_data.shape) \n",
    "print(train_data.shape, train_label.shape, test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e284aee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11982, 785)\n",
      "[-1 -1 -1 ...  1 -1  1]\n",
      "(1984, 785)\n",
      "[-1 -1 -1 ... -1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "# prepare digits '3' and '8' for Ridge regression\n",
    "digit_train_index = np.logical_or(train_label == 3, train_label == 8)\n",
    "X_train = train_data[digit_train_index]\n",
    "y_train = train_label[digit_train_index]\n",
    "digit_test_index = np.logical_or(test_label == 3, test_label == 8)\n",
    "X_test = test_data[digit_test_index]\n",
    "y_test = test_label[digit_test_index]\n",
    "# add a constant column of '1' to accomodate the bias\n",
    "X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1), dtype=X_train.dtype)))\n",
    "X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1), dtype=X_test.dtype)))\n",
    "# convert labels: '3' => -1, '8' => +1\n",
    "CUTOFF = 5 # any number between '3' and '8'\n",
    "y_train = np.sign(y_train-CUTOFF)\n",
    "y_test = np.sign(y_test-CUTOFF)\n",
    "print(X_train.shape)\n",
    "print(y_train)\n",
    "print(X_test.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82e589",
   "metadata": {},
   "source": [
    "**Solving by Closed-form**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5782baf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error on training data for the closed-form Ridge regression solution: 0.20013\n",
      "Ridge regression accuracy on training data for the closed-form solution: 96.77016%\n",
      "Mean square error on test data for the closed-form Ridge regression solution: 0.22014\n",
      "Ridge regression accuracy on test data for the closed-form solution: 96.01815%\n"
     ]
    }
   ],
   "source": [
    "# use the Ridge regression closed-form solution\n",
    "I = np.eye(X_train.shape[1])\n",
    "I[0][0] = 0 \n",
    "alpha = 1\n",
    "# refer to the closed-form solution\n",
    "w = np.linalg.inv(X_train.T @ X_train + alpha*I) @ X_train.T @ y_train\n",
    "# calculate the mean square error and Ridge regression accuracy on the training set\n",
    "predict = X_train @ w \n",
    "error = np.sum((predict - y_train)*(predict - y_train))/X_train.shape[0]\n",
    "print(f'Mean square error on training data for the closed-form Ridge regression solution: {error:.5f}')\n",
    "accuracy = np.count_nonzero(np.equal(np.sign(predict),y_train))/y_train.size*100.0\n",
    "print(f'Ridge regression accuracy on training data for the closed-form solution: {accuracy:.5f}%')\n",
    "\n",
    "# calculate the mean square error and Ridge regression accuracy on the test set\n",
    "predict = X_test @ w \n",
    "error = np.sum((predict - y_test)*(predict - y_test))/X_test.shape[0]\n",
    "print(f'Mean square error on test data for the closed-form Ridge regression solution: {error:.5f}')\n",
    "accuracy = np.count_nonzero(np.equal(np.sign(predict),y_test))/y_test.size*100.0\n",
    "print(f'Ridge regression accuracy on test data for the closed-form solution: {accuracy:.5f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78375b98",
   "metadata": {},
   "source": [
    "**Solving by Iterative Manner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0528f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve ridge regression using gradient descent \n",
    "class Optimizer():\n",
    "    def __init__(self, lr, alpha, annealing_rate, batch_size, max_epochs):\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.annealing_rate = annealing_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "# X[N,d]: input features; y[N]: output targets; op: hyper-parameters for optimzer \n",
    "def ridge_regression_gd2(X, y, X2, y2, op):\n",
    "    n = X.shape[0]   # number of samples\n",
    "    w = np.zeros(X.shape[1])  # initialization\n",
    "    lr = op.lr\n",
    "    errorsA = np.zeros(op.max_epochs)\n",
    "    errorsB = np.zeros(op.max_epochs)\n",
    "    errorsC = np.zeros(op.max_epochs)\n",
    "    for epoch in range(op.max_epochs):\n",
    "        indices = np.random.permutation(n)  #randomly shuffle data indices\n",
    "        for batch_start in range(0, n, op.batch_size):\n",
    "            X_batch = X[indices[batch_start:batch_start + op.batch_size]]\n",
    "            y_batch = y[indices[batch_start:batch_start + op.batch_size]]\n",
    "            # vectorization to compute gradients for a whole mini-batch (see the above formula)\n",
    "            w_grad = (-2/y_batch.size)*X_batch.T.dot((y_batch - X_batch@w)) + op.alpha*w \n",
    "            w -= lr * w_grad / X_batch.shape[0]\n",
    "        \n",
    "        diff = X @ w - y  # prediction difference\n",
    "        errors[epoch] = np.sum(diff*diff)/n\n",
    "        \n",
    "        # for learning curve A\n",
    "        predict = np.sign(X @ w)\n",
    "        errorsA[epoch] = np.count_nonzero(np.equal(predict,y))/y.size \n",
    "        # for learning curve B\n",
    "        predict2 = np.sign(X2 @ w)\n",
    "        errorsB[epoch] = np.count_nonzero(np.equal(predict2,y2))/y2.size\n",
    "        lr *= op.annealing_rate\n",
    "        print(f'epoch={epoch}: the mean square error is ({errorsA[epoch]:.3f},{errorsB[epoch]:.3f})')\n",
    "    return w, errorsA, errorsB, errorsC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40389cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: the mean square error is (0.894,0.903)\n",
      "epoch=1: the mean square error is (0.907,0.915)\n",
      "epoch=2: the mean square error is (0.916,0.925)\n",
      "epoch=3: the mean square error is (0.923,0.935)\n",
      "epoch=4: the mean square error is (0.928,0.941)\n",
      "epoch=5: the mean square error is (0.933,0.945)\n",
      "epoch=6: the mean square error is (0.937,0.950)\n",
      "epoch=7: the mean square error is (0.940,0.953)\n",
      "epoch=8: the mean square error is (0.942,0.955)\n",
      "epoch=9: the mean square error is (0.943,0.957)\n",
      "epoch=10: the mean square error is (0.944,0.958)\n",
      "epoch=11: the mean square error is (0.946,0.959)\n",
      "epoch=12: the mean square error is (0.949,0.958)\n",
      "epoch=13: the mean square error is (0.950,0.960)\n",
      "epoch=14: the mean square error is (0.950,0.960)\n",
      "epoch=15: the mean square error is (0.951,0.960)\n",
      "epoch=16: the mean square error is (0.952,0.962)\n",
      "epoch=17: the mean square error is (0.952,0.962)\n",
      "epoch=18: the mean square error is (0.952,0.963)\n",
      "epoch=19: the mean square error is (0.953,0.962)\n",
      "Mean square error on training data for the Iterative Ridge regression solution: 0.24336\n",
      "Mean square error on test data for the Iterative Ridge regression solution: 0.22582\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "op = Optimizer(lr=0.01, alpha=0.1, annealing_rate=0.99, batch_size=50, max_epochs=20)\n",
    "w, A, B, C = ridge_regression_gd2(X_train, y_train, X_test, y_test, op)\n",
    "#predicting for training data\n",
    "predict1 = X_train @ w\n",
    "#MSE for #MSE for training data\n",
    "print(\"Mean square error on training data for the Iterative Ridge regression solution: %.5f\" % mean_squared_error(y_train, predict1))\n",
    "#predicting for testing data\n",
    "predict2 = X_test @ w\n",
    "#MSE for testing data\n",
    "print(\"Mean square error on test data for the Iterative Ridge regression solution: %.5f\" % mean_squared_error(y_test, predict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a411a92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error on training data for the Iterative Ridge regression solution: 0.21094\n",
      "Mean square error on test data for the Iterative Ridge regression solution: 0.21064\n"
     ]
    }
   ],
   "source": [
    "#for testing purpose please ignore the block\n",
    "\n",
    "#hyperparameters\n",
    "#op = RidgeRegression(iterations = 1000, learning_rate = 0.01, L2_penality = 2)\n",
    "#training model\n",
    "#op.fit(X_train, y_train)\n",
    "#predicting for training data\n",
    "#predict1 = op.predict(X_train)\n",
    "#MSE for #MSE for testing data\n",
    "#print(\"Mean square error on training data for the Iterative Ridge regression solution: %.5f\" % mean_squared_error(y_train, predict1))\n",
    "#predicting for testing data\n",
    "#predict2 = op.predict(X_test)\n",
    "#MSE for testing data\n",
    "#print(\"Mean square error on test data for the Iterative Ridge regression solution: %.5f\" % mean_squared_error(y_test, predict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e08b527a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error on training data for Linear regression: 0.19615\n",
      "Mean squared error on test data for Linear regression: 1.86130\n",
      "Mean squared error on training data for Ridge regression: 0.20013\n",
      "Mean squared error on test data for Ridge regression: 0.22014\n"
     ]
    }
   ],
   "source": [
    "# Comparison with Linear regression\n",
    "\n",
    "# Create linear regression object\n",
    "l_regr = linear_model.LinearRegression()\n",
    "# Train the model using the training set\n",
    "l_regr.fit(X_train, y_train)\n",
    "# Make predictions using the same training set\n",
    "predict1 = l_regr.predict(X_train)\n",
    "print(\"Mean squared error on training data for Linear regression: %.5f\" % mean_squared_error(y_train, predict1))\n",
    "# Make predictions using the test set\n",
    "predict2 = l_regr.predict(X_test)\n",
    "print(\"Mean squared error on test data for Linear regression: %.5f\" % mean_squared_error(y_test, predict2))\n",
    "\n",
    "# Create Ridge regression object\n",
    "Ridge = linear_model.Ridge()\n",
    "# Train the model using the training set\n",
    "Ridge.fit(X_train, y_train)\n",
    "# Make predictions using the same training set\n",
    "predict3 = Ridge.predict(X_train)\n",
    "print(\"Mean squared error on training data for Ridge regression: %.5f\" % mean_squared_error(y_train, predict3))\n",
    "# Make predictions using the test set\n",
    "predict4 = Ridge.predict(X_test)\n",
    "print(\"Mean squared error on test data for Ridge regression: %.5f\" % mean_squared_error(y_test, predict4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb0013",
   "metadata": {},
   "source": [
    "### **Problem 2.2:**\n",
    "\n",
    "Use LASSO to solve the regression problem in Example 2.1 as well as the classification problem in Example 2.2, compare the results of LASSO with those of linear regression and Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c891e4e",
   "metadata": {},
   "source": [
    "**Example 2.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "002997ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 28.46465\n"
     ]
    }
   ],
   "source": [
    "lasso = linear_model.Lasso(alpha = 1)\n",
    "lasso.fit(data_wb, target)\n",
    "predict = lasso.predict(data_wb)\n",
    "print(\"Mean squared error: %.5f\" % mean_squared_error(target, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa82611",
   "metadata": {},
   "source": [
    "**Example 2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d075c650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error on training data solution: 0.99945\n",
      "Lasso accuracy on training data solution: 51.17%\n",
      "Mean squared error on test data solution: 0.99970\n",
      "Lasso accuracy on test data solution: 8.43%\n"
     ]
    }
   ],
   "source": [
    "lasso = linear_model.Lasso(alpha = 1)\n",
    "lasso.fit(X_train,y_train)\n",
    "predict1 = lasso.predict(X_train)\n",
    "print(\"Mean squared error on training data solution: %.5f\" % mean_squared_error(y_train, predict1))\n",
    "accuracy = np.count_nonzero(np.equal(np.sign(predict1),y_train))/y_train.size*100.0\n",
    "print(f'Lasso accuracy on training data solution: {accuracy:.2f}%')\n",
    "\n",
    "predict2 = lasso.predict(X_test)\n",
    "print(\"Mean squared error on test data solution: %.5f\" % mean_squared_error(y_test, predict2))\n",
    "accuracy = np.count_nonzero(np.equal(np.sign(predict2),y_test))/y_train.size*100.0\n",
    "print(f'Lasso accuracy on test data solution: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94818820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
